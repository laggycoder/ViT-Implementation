{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPT3s/5GTguCVYpOiyLK64P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"rwLI0aZaETb0","executionInfo":{"status":"ok","timestamp":1747167036994,"user_tz":-330,"elapsed":7395,"user":{"displayName":"Ishan Manindra Gupta ee24b109","userId":"18072811226322746429"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as T\n","import numpy as np\n","from PIL import Image"]},{"cell_type":"code","source":["class SelfAttention(nn.Module):\n","    def __init__(self, embedding_dim=768, key_dim=64):\n","        super(SelfAttention, self).__init__()\n","\n","        self.embedding_dim = embedding_dim\n","        self.key_dim = key_dim\n","\n","        self.W = nn.Parameter(torch.randn(embedding_dim, 3*key_dim))\n","\n","    def forward(self, x):\n","        key_dim = self.key_dim\n","\n","\n","        qkv = torch.matmul(x, self.W)\n","\n","        q = qkv[:, :, :key_dim]\n","        k = qkv[:, :, key_dim:key_dim*2 ]\n","        v = qkv[:, :, key_dim*2:]\n","\n","\n","        k_T = torch.transpose(k, -2, -1)\n","        dot_products = torch.matmul(q, k_T)\n","\n","\n","        scaled_dot_products = dot_products / np.sqrt(key_dim)\n","\n","        attention_weights = F.softmax(scaled_dot_products, dim=1)\n","\n","        weighted_values = torch.matmul(attention_weights, v)\n","\n","        return weighted_values"],"metadata":{"id":"PRJSfC70O7qb","executionInfo":{"status":"ok","timestamp":1747167036998,"user_tz":-330,"elapsed":1,"user":{"displayName":"Ishan Manindra Gupta ee24b109","userId":"18072811226322746429"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class MultiHeadSelfAttention(nn.Module):\n","    def __init__(self, embedding_dim=768, num_heads=12):\n","        super(MultiHeadSelfAttention, self).__init__()\n","\n","        self.num_heads = num_heads\n","        self.embedding_dim = embedding_dim\n","\n","        assert embedding_dim % num_heads == 0\n","        self.key_dim = embedding_dim // n_head\n","\n","\n","        self.attention_list = [SelfAttention(embedding_dim, self.key_dim) for _ in range(num_heads)]\n","        self.multi_head_attention = nn.ModuleList(self.attention_list)\n","\n","\n","        self.W = nn.Parameter(torch.randn(num_heads * self.key_dim, embedding_dim))\n","\n","    def forward(self, x):\n","\n","        attention_scores = [attention(x) for attention in self.multi_head_attention]\n","\n","        Z = torch.cat(attention_scores, -1)\n","\n","        attention_score = torch.matmul(Z, self.W)\n","\n","        return attention_score"],"metadata":{"id":"ZbBadPKZPBTu","executionInfo":{"status":"ok","timestamp":1747167037002,"user_tz":-330,"elapsed":2,"user":{"displayName":"Ishan Manindra Gupta ee24b109","userId":"18072811226322746429"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class MultiLayerPerceptron(nn.Module):\n","    def __init__(self, embedding_dim=768, hidden_dim=3072):\n","        super(MultiLayerPerceptron, self).__init__()\n","\n","        self.mlp = nn.Sequential(\n","                            nn.Linear(embedding_dim, hidden_dim),\n","                            nn.GELU(),\n","                            nn.Linear(hidden_dim, embedding_dim)\n","                   )\n","\n","    def forward(self, x):\n","        x = self.mlp(x)\n","        return x"],"metadata":{"id":"BQFCMolIPFRu","executionInfo":{"status":"ok","timestamp":1747167037005,"user_tz":-330,"elapsed":1,"user":{"displayName":"Ishan Manindra Gupta ee24b109","userId":"18072811226322746429"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, embedding_dim=768, num_heads=12, hidden_dim=3072, dropout_prob=0.1):\n","        super(TransformerEncoder, self).__init__()\n","\n","        self.MSA = MultiHeadSelfAttention(embedding_dim, num_heads)\n","        self.MLP = MultiLayerPerceptron(embedding_dim, hidden_dim)\n","\n","        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n","        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n","\n","        self.dropout1 = nn.Dropout(p=dropout_prob)\n","        self.dropout2 = nn.Dropout(p=dropout_prob)\n","        self.dropout3 = nn.Dropout(p=dropout_prob)\n","\n","    def forward(self, x):\n","\n","        out_1 = self.dropout1(x)\n","\n","        out_2 = self.layer_norm1(out_1)\n","\n","        msa_out = self.MSA(out_2)\n","\n","        out_3 = self.dropout2(msa_out)\n","\n","        res_out = x + out_3\n","\n","        out_4 = self.layer_norm2(res_out)\n","\n","        mlp_out = self.MLP(out_4)\n","\n","        out_5 = self.dropout3(mlp_out)\n","\n","        output = res_out + out_5\n","\n","        return output"],"metadata":{"id":"uuIIxZFsPHuG","executionInfo":{"status":"ok","timestamp":1747167037018,"user_tz":-330,"elapsed":12,"user":{"displayName":"Ishan Manindra Gupta ee24b109","userId":"18072811226322746429"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class MLPHead(nn.Module):\n","    def __init__(self, embedding_dim=768, num_classes=10, fine_tune=False):\n","        super(MLPHead, self).__init__()\n","        self.num_classes = num_classes\n","\n","        if not fine_tune:\n","            self.mlp_head = nn.Sequential(\n","                                    nn.Linear(embedding_dim, 3072),\n","                                    nn.Tanh(),\n","                                    nn.Linear(3072, num_classes)\n","                            )\n","        else:\n","\n","            self.mlp_head = nn.Linear(embedding_dim, num_classes)\n","\n","    def forward(self, x):\n","        x = self.mlp_head(x)\n","        return x"],"metadata":{"id":"LTeOE2-KPJvG","executionInfo":{"status":"ok","timestamp":1747167037022,"user_tz":-330,"elapsed":2,"user":{"displayName":"Ishan Manindra Gupta ee24b109","userId":"18072811226322746429"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class VisionTransformer(nn.Module):\n","    def __init__(self, patch_size=16, image_size=224, channel_size=3,\n","                     num_layers=12, embedding_dim=768, num_heads=12, hidden_dim=3072,\n","                            dropout_prob=0.1, num_classes=10, pretrain=True):\n","        super(VisionTransformer, self).__init__()\n","\n","        self.patch_size = patch_size\n","        self.channel_size = channel_size\n","        self.num_layers = num_layers\n","        self.embedding_dim = embedding_dim\n","        self.num_heads = num_heads\n","        self.hidden_dim = hidden_dim\n","        self.dropout_prob = dropout_prob\n","        self.num_classes = num_classes\n","\n","        self.num_patches = int(image_size ** 2 / patch_size ** 2)\n","        self.patch_embedding = nn.Linear(patch_size * patch_size * channel_size, embedding_dim)\n","        self.pos_embedding = nn.Parameter(torch.randn(self.num_patches + 1, embedding_dim))\n","        self.class_token = nn.Parameter(torch.rand(1, embedding_dim))\n","\n","        transformer_encoder_list = [\n","            TransformerEncoder(embedding_dim, num_heads, hidden_dim, dropout_prob)\n","                    for _ in range(num_layers)]\n","        self.transformer_encoder_layers = nn.Sequential(*transformer_encoder_list)\n","\n","        self.mlp_head = MLPHead(embedding_dim, num_classes)\n","\n","    def forward(self, x):\n","        # get patch size and channel size\n","        P, C = self.patch_size, self.channel_size\n","\n","        # split image into patches\n","        patches = x.unfold(1, C, C).unfold(2, P, P).unfold(3, P, P)\n","        patches = patches.contiguous().view(patches.size(0), -1, C * P * P).float()\n","\n","        # linearly embed patches\n","        patch_embeddings = self.patch_embedding(patches)\n","\n","\n","        # add class token\n","        batch_size = patch_embeddings.shape[0]\n","        patch_embeddings = torch.cat((self.class_token.repeat(batch_size, 1, 1), patch_embeddings), 1)\n","\n","        # add positional embedding\n","        patch_embeddings = patch_embeddings + self.pos_embedding\n","\n","        # feed patch embeddings into a stack of Transformer encoders\n","        transformer_encoder_output = self.transformer_encoder_layers(patch_embeddings)\n","\n","        # extract [class] token from encoder output\n","        output_class_token = transformer_encoder_output[:, 0]\n","\n","        # pass token through mlp head for classification\n","        y = self.mlp_head(output_class_token)\n","\n","        return y"],"metadata":{"id":"7Id7zTS7PNTt","executionInfo":{"status":"ok","timestamp":1747167037033,"user_tz":-330,"elapsed":9,"user":{"displayName":"Ishan Manindra Gupta ee24b109","userId":"18072811226322746429"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import torchvision\n","from torchvision import datasets\n","from torch.utils.data import DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torchvision.models as models\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler"],"metadata":{"id":"wjxPwAbJPx-N","executionInfo":{"status":"ok","timestamp":1747167037044,"user_tz":-330,"elapsed":9,"user":{"displayName":"Ishan Manindra Gupta ee24b109","userId":"18072811226322746429"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["image_size = 224\n","transform = T.Compose([\n","    T.Resize(image_size),\n","    T.RandomHorizontalFlip(),\n","    T.RandomRotation(15),\n","    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","    T.ToTensor(),\n","    T.Normalize((0.5,), (0.5,))\n","])\n","\n","torchvision.datasets.CIFAR10.url=\"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n","\n","trainset = datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n","testset = datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n","\n","\n","classes = trainset.classes\n","\n","trainset = torch.utils.data.Subset(trainset, list(range(20000)))\n","testset = torch.utils.data.Subset(testset, list(range(4000)))\n","\n","classes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0YkQa7_P1en","executionInfo":{"status":"ok","timestamp":1747167039553,"user_tz":-330,"elapsed":2497,"user":{"displayName":"Ishan Manindra Gupta ee24b109","userId":"18072811226322746429"}},"outputId":"96994e3d-6599-4399-a339-36f0a3200186"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['airplane',\n"," 'automobile',\n"," 'bird',\n"," 'cat',\n"," 'deer',\n"," 'dog',\n"," 'frog',\n"," 'horse',\n"," 'ship',\n"," 'truck']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["batch_size = 64\n","\n","valid_size = 0.2\n","\n","train_size = len(trainset)\n","indices = list(range(train_size))\n","np.random.shuffle(indices)\n","split = int(np.floor(valid_size * train_size))\n","train_idx, valid_idx = indices[split:], indices[:split]\n","\n","train_sampler = SubsetRandomSampler(train_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","train_loader = DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)\n","valid_loader = DataLoader(trainset, batch_size=batch_size, sampler=valid_sampler)\n","test_loader = DataLoader(testset, batch_size=batch_size)"],"metadata":{"id":"y_SldUcvP9it","executionInfo":{"status":"ok","timestamp":1747167039566,"user_tz":-330,"elapsed":11,"user":{"displayName":"Ishan Manindra Gupta ee24b109","userId":"18072811226322746429"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AL25CZ4RQA6G","executionInfo":{"status":"ok","timestamp":1747167039582,"user_tz":-330,"elapsed":14,"user":{"displayName":"Ishan Manindra Gupta ee24b109","userId":"18072811226322746429"}},"outputId":"2638bc94-c400-4997-c6ee-a3beceed653a"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["image_size = 224; channel_size = 3\n","n_class = 10\n","dropout_prob = 0.1\n","n_layer = 12; embedding_dim = 768; n_head = 12; hidden_dim=3072\n","patch_size = 16\n","\n","vision_transformer = VisionTransformer(patch_size, image_size, channel_size,\n","                            n_layer, embedding_dim, n_head, hidden_dim, dropout_prob, n_class).to(device)"],"metadata":{"id":"bfDMBusUQFcU","executionInfo":{"status":"ok","timestamp":1747167040773,"user_tz":-330,"elapsed":1189,"user":{"displayName":"Ishan Manindra Gupta ee24b109","userId":"18072811226322746429"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","\n","optimizer = optim.AdamW(vision_transformer.parameters(), lr=1e-4, weight_decay=1e-4)\n","\n","scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-4,\n","                                          steps_per_epoch=len(train_loader),\n","                                          epochs=20, pct_start=0.1)"],"metadata":{"id":"UwrK5sNwQIUs","executionInfo":{"status":"ok","timestamp":1747167040788,"user_tz":-330,"elapsed":9,"user":{"displayName":"Ishan Manindra Gupta ee24b109","userId":"18072811226322746429"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","num_epochs = 20\n","\n","for epoch in range(num_epochs):\n","    model = vision_transformer\n","    model.train()\n","    train_loss = 0\n","    correct, total = 0, 0\n","\n","    for images, labels in tqdm(train_loader):\n","        images, labels = images.to(device), labels.to(device)\n","\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","        train_loss += loss.item()\n","\n","        _, predicted = outputs.max(1)\n","        correct += (predicted == labels).sum().item()\n","        total += labels.size(0)\n","\n","    train_accuracy = 100 * correct / total\n","    avg_train_loss = train_loss / len(train_loader)\n","\n","\n","    model.eval()\n","    val_loss, correct, total = 0, 0, 0\n","\n","    with torch.no_grad():\n","        for images, labels in valid_loader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            val_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            correct += (predicted == labels).sum().item()\n","            total += labels.size(0)\n","\n","    val_accuracy = 100 * correct / total\n","    avg_val_loss = val_loss / len(valid_loader)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tPS7I8mPQMsd","outputId":"4b225be3-c974-4866-f7ec-3e992915f730"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/250 [00:00<?, ?it/s]"]}]}]}