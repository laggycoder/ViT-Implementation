# ViT-Implementation

**AI Club DC Mini Project**

This project is a step-by-step journey into modern deep learning architectures, with a special focus on understanding and implementing **Vision Transformers (ViT)** using PyTorch. The tasks below are designed to build foundational knowledge and practical coding skills.

---

## 🚀 Tasks Overview

### ✅ Task 1: Convolutional Neural Networks (CNNs)

- **Objective:** Understand and implement a basic CNN.
- **What to do:**
  - Study CNN architecture fundamentals (convolutional layers, pooling, activation functions).
  - Implement a simple CNN from scratch in PyTorch (e.g., on MNIST or CIFAR-10).

---

### ✅ Task 2: Attention Mechanism & Transformer Encoder-Decoder

1. **Paper Reading:**
   - Read and understand the landmark paper: [**Attention Is All You Need**](https://arxiv.org/pdf/1706.03762.pdf)

2. **Practical Exploration:**
   - Go through a blog/tutorial where an **encoder-decoder transformer model** is implemented from scratch.
   - Suggested blog: *[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)* (or choose your preferred one).

---

### ✅ Task 3: Vision Transformer (ViT)

1. **Paper Reading:**
   - Dive into the original Vision Transformer paper: [**An Image is Worth 16x16 Words**](https://arxiv.org/pdf/2010.11929.pdf)

2. **Implementation:**
   - Code the **ViT architecture from scratch** using **only PyTorch** (no high-level transformer libraries like Hugging Face).
   - Understand and build:
     - Patch embeddings
     - Positional encodings
     - Transformer encoder blocks
     - Classification head

---

## 📁 Repository Structure (Implemented)

```plaintext
ViT-Implementation/
├── task1_cnn/
│   └── cnn_model.ipynb
├── task2_transformer/
│   └── encoder_decoder_from_scratch.ipynb
├── task3_vit/
│   └── vit_from_scratch.ipynb
└── README.md
```
